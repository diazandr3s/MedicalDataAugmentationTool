{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import utils.io.image\n",
    "import tensorflow as tf\n",
    "import tensorflow_train.utils.tensorflow_util\n",
    "from tensorflow_train.data_generator import DataGenerator\n",
    "from tensorflow_train.losses.semantic_segmentation_losses import softmax_cross_entropy_with_logits\n",
    "from tensorflow_train.train_loop import MainLoopBase\n",
    "import utils.sitk_image\n",
    "from utils.segmentation.segmentation_test import SegmentationTest\n",
    "from utils.segmentation.segmentation_statistics import SegmentationStatistics\n",
    "from utils.segmentation.metrics import DiceMetric\n",
    "from dataset import Dataset\n",
    "from network import network_scn\n",
    "from tensorflow_train.utils.summary_handler import SummaryHandler, create_summary_placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainLoop(MainLoopBase): \n",
    "    def __init__(self, modality, cv, path_weights):\n",
    "        super().__init__()\n",
    "        self.modality = modality\n",
    "        self.cv = cv\n",
    "        self.batch_size = 1\n",
    "        self.load_model_filename = path_weights\n",
    "        self.learning_rates = [0.00001, 0.000001]\n",
    "        self.learning_rate_boundaries = [20000]\n",
    "        self.max_iter = 40000\n",
    "        self.test_iter = 5000\n",
    "        self.disp_iter = 100\n",
    "        self.snapshot_iter = self.test_iter\n",
    "        self.test_initialization = False\n",
    "        self.current_iter = 0\n",
    "        self.reg_constant = 0.0001\n",
    "        self.num_labels = 8\n",
    "        self.data_format = 'channels_first'\n",
    "        self.channel_axis = 1\n",
    "        self.save_debug_images = True\n",
    "\n",
    "        self.has_validation_groundtruth = False\n",
    "        self.base_folder = 'mmwhs_dataset'\n",
    "        self.image_size = [64, 64, 64]\n",
    "        \n",
    "        if modality == 'ct':\n",
    "            self.image_spacing = [3, 3, 3]\n",
    "        else:\n",
    "            self.image_spacing = [4, 4, 4]\n",
    "        self.input_gaussian_sigma = 1.0\n",
    "        self.label_gaussian_sigma = 1.0\n",
    "\n",
    "        self.output_folder = 'results/scn_' + modality + '_' + str(cv) + '/' + self.output_folder_timestamp()\n",
    "\n",
    "        self.dataset = Dataset(self.image_size,\n",
    "                               self.image_spacing,\n",
    "                               self.base_folder,\n",
    "                               self.cv,\n",
    "                               self.modality,\n",
    "                               self.input_gaussian_sigma,\n",
    "                               self.label_gaussian_sigma,\n",
    "                               self.data_format,\n",
    "                               self.save_debug_images)\n",
    "\n",
    "        self.dataset_train = self.dataset.dataset_train()\n",
    "        self.dataset_val = self.dataset.dataset_val()\n",
    "        self.files_to_copy = ['main.py', 'network.py', 'dataset.py']\n",
    "        self.dice_names = list(map(lambda x: 'dice_{}'.format(x), range(self.num_labels)))\n",
    "        self.additional_summaries_placeholders_val = dict([(name, create_summary_placeholder(name)) for name in self.dice_names])\n",
    "        self.loss_function = softmax_cross_entropy_with_logits\n",
    "        self.network = network_scn\n",
    "\n",
    "    def initNetworks(self):\n",
    "        network_image_size = self.image_size\n",
    "\n",
    "        if self.data_format == 'channels_first':\n",
    "            data_generator_entries = OrderedDict([('data', [1] + network_image_size),\n",
    "                                                  ('mask', [self.num_labels] + network_image_size)])\n",
    "        else:\n",
    "            data_generator_entries = OrderedDict([('data', network_image_size + [1]),\n",
    "                                                  ('mask', network_image_size + [self.num_labels])])\n",
    "\n",
    "        # create model with shared weights between train and val\n",
    "        training_net = tf.make_template('net', self.network)\n",
    "\n",
    "        # build train graph\n",
    "        self.train_queue = DataGenerator(self.dataset_train, self.coord, data_generator_entries, batch_size=self.batch_size)\n",
    "        data, mask = self.train_queue.dequeue()\n",
    "        prediction, _, _ = training_net(data, num_labels=self.num_labels, is_training=True, data_format=self.data_format)\n",
    "        # losses\n",
    "        self.loss_net = self.loss_function(labels=mask, logits=prediction, data_format=self.data_format)\n",
    "\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            if self.reg_constant > 0:\n",
    "                reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "                self.loss_reg = self.reg_constant * tf.add_n(reg_losses)\n",
    "                self.loss = self.loss_net + self.loss_reg\n",
    "            else:\n",
    "                self.loss = self.loss_net\n",
    "\n",
    "        self.train_losses = OrderedDict([('loss', self.loss_net), ('loss_reg', self.loss_reg)])\n",
    "\n",
    "        # solver\n",
    "        global_step = tf.Variable(self.current_iter)\n",
    "        learning_rate = tf.train.piecewise_constant(global_step, self.learning_rate_boundaries, self.learning_rates)\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(self.loss, global_step=global_step)\n",
    "\n",
    "        # build val graph\n",
    "        val_placeholders = tensorflow_train.utils.tensorflow_util.create_placeholders(data_generator_entries, shape_prefix=[1])\n",
    "        self.data_val = val_placeholders['data']\n",
    "        self.prediction_val, self.local_prediction_val, self.spatial_prediction_val = training_net(self.data_val, num_labels=self.num_labels, is_training=False, data_format=self.data_format)\n",
    "        self.prediction_softmax_val = self.prediction_val / tf.reduce_sum(self.prediction_val, axis=1, keepdims=True)\n",
    "\n",
    "        if self.has_validation_groundtruth:\n",
    "            self.mask_val = val_placeholders['mask']\n",
    "            # losses\n",
    "            self.loss_val = self.loss_function(labels=self.mask_val, logits=self.prediction_val, data_format=self.data_format)\n",
    "            self.val_losses = OrderedDict([('loss', self.loss_val), ('loss_reg', self.loss_reg)])\n",
    "\n",
    "    def test(self):\n",
    "        print('Testing...')\n",
    "        channel_axis = 0\n",
    "        if self.data_format == 'channels_last':\n",
    "            channel_axis = 3\n",
    "        labels = list(range(self.num_labels))\n",
    "        segmentation_test = SegmentationTest(labels,\n",
    "                                             channel_axis=channel_axis,\n",
    "                                             interpolator='cubic',\n",
    "                                             largest_connected_component=True,\n",
    "                                             all_labels_are_connected=True)\n",
    "        segmentation_statistics = SegmentationStatistics(labels,\n",
    "                                                         self.output_folder_for_current_iteration(),\n",
    "                                                         metrics={'dice': DiceMetric()})\n",
    "        num_entries = self.dataset_val.num_entries()\n",
    "        for i in range(num_entries):\n",
    "            dataset_entry = self.dataset_val.get_next()\n",
    "            current_id = dataset_entry['id']['image_id']\n",
    "            datasources = dataset_entry['datasources']\n",
    "            generators = dataset_entry['generators']\n",
    "            transformations = dataset_entry['transformations']\n",
    "            if self.has_validation_groundtruth:\n",
    "                feed_dict = {self.data_val: np.expand_dims(generators['data'], axis=0),\n",
    "                             self.mask_val: np.expand_dims(generators['mask'], axis=0)}\n",
    "                # run loss and update loss accumulators\n",
    "                run_tuple = self.sess.run((self.prediction_val, self.local_prediction_val, self.spatial_prediction_val, self.loss_val) + self.val_loss_aggregator.get_update_ops(),\n",
    "                                          feed_dict=feed_dict)\n",
    "            else:\n",
    "                feed_dict = {self.data_val: np.expand_dims(generators['data'], axis=0)}\n",
    "                # run loss and update loss accumulators\n",
    "                run_tuple = self.sess.run((self.prediction_val,), feed_dict=feed_dict)\n",
    "\n",
    "            # print(iv[0].decode())\n",
    "            prediction = np.squeeze(run_tuple[0], axis=0)\n",
    "            #local_prediction = np.squeeze(run_tuple[1], axis=0)\n",
    "            #spatial_prediction = np.squeeze(run_tuple[2], axis=0)\n",
    "            input = datasources['image']\n",
    "            transformation = transformations['data']\n",
    "            prediction_labels, prediction_sitk = segmentation_test.get_label_image(prediction, input, self.image_spacing, transformation, return_transformed_sitk=True)\n",
    "            utils.io.image.write(prediction_labels, self.output_file_for_current_iteration(current_id + '.mha'))\n",
    "            utils.io.image.write_np(prediction, self.output_file_for_current_iteration(current_id + '_prediction.mha'))\n",
    "            #utils.io.image.write_np(local_prediction, self.output_file_for_current_iteration(current_id + '_local_prediction.mha'))\n",
    "            #utils.io.image.write_np(spatial_prediction, self.output_file_for_current_iteration(current_id + '_spatial_prediction.mha'))\n",
    "            if self.has_validation_groundtruth:\n",
    "                groundtruth = datasources['mask']\n",
    "                segmentation_statistics.add_labels(current_id, prediction_labels, groundtruth)\n",
    "            tensorflow_train.utils.tensorflow_util.print_progress_bar(i, num_entries, prefix='Testing ', suffix=' complete')\n",
    "\n",
    "        # finalize loss values\n",
    "        if self.has_validation_groundtruth:\n",
    "            segmentation_statistics.finalize()\n",
    "            dice_list = segmentation_statistics.get_metric_mean_list('dice')\n",
    "            dice_dict = OrderedDict(list(zip(self.dice_names, dice_list)))\n",
    "            self.val_loss_aggregator.finalize(self.current_iter, summary_values=dice_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For MRI images:\n",
    "#path_weights = './media1/experiments/mmwhs/scn_mr_0/2019-06-23_01-20-19/weights/model-40000'\n",
    "\n",
    "# For CTA images:\n",
    "path_weights = './media1/experiments/mmwhs/scn_ct_0/2019-06-21_17-19-44/weights/model-40000'\n",
    "\n",
    "object_mainLoop = MainLoop('ct', 0, path_weights)\n",
    "object_mainLoop.run_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (envGUT)",
   "language": "python",
   "name": "envgut"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
